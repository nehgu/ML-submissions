{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module has been deprecated in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126837, 52)\n",
      "(31709, 51)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas_ml as pdml\n",
    "import numpy as np\n",
    "from sklearn import cross_validation, metrics   #Additional scklearn functions\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_extraction import DictVectorizer as DV\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4\n",
    "\n",
    "pd.set_option('display.max_rows', 8)\n",
    "pd.set_option('display.max_rows', 6)\n",
    "\n",
    "train_df = pd.read_csv('data3.csv',header=0, index_col=0)\n",
    "test_df = pd.read_csv('quiz.csv',header=0, index_col=0)\n",
    "print(train_df.shape)\n",
    "# convert to pdml.ModelFrame\n",
    "#train_df = pdml.ModelFrame(train_df,target='label')\n",
    "#train_df=train_df.dropna()\n",
    "#test_df = pdml.ModelFrame(test_df)\n",
    "#test_df=test_df.dropna()\n",
    "#print (train_df)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin\n",
    "class DataFrameImputer(TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        self.fill = pd.Series([X[c].value_counts().index[0]\n",
    "            if X[c].dtype == np.dtype('O') else X[c].median() for c in X],\n",
    "            index=X.columns)\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        return X.fillna(self.fill)\n",
    "\n",
    "feature_columns_to_use = ([x for x in train_df.columns])\n",
    "feature_columns_to_use.pop(-1)\n",
    "\n",
    "numeric_cols = ['59','60']\n",
    "nonnumeric_columns = feature_columns_to_use[0:46]+feature_columns_to_use[48:]\n",
    "\n",
    "# Join the features from train and test together before imputing missing values,\n",
    "# in case their distribution is slightly different\n",
    "big_X = train_df[feature_columns_to_use].append(test_df[feature_columns_to_use])\n",
    "big_X_imputed = DataFrameImputer().fit_transform(big_X)\n",
    "#print(big_X_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.00000000e+00   2.50000000e+01   1.00000000e+00   7.00000000e+00\n",
      "   1.00000000e+00   0.00000000e+00   2.50000000e+01   0.00000000e+00\n",
      "   4.00000000e+00   7.00000000e+01   0.00000000e+00   7.90000000e+02\n",
      "   2.00000000e+00   8.70000000e+01   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   1.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   2.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   1.00000000e+00   1.01000000e+02\n",
      "   9.90000000e+01   2.08800000e+03   8.00000000e+01   4.90000000e+01\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "# To handle categorical features, we need to change\n",
    "# them to columns of integer values.\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "le = LabelEncoder()\n",
    "for feature in nonnumeric_columns:\n",
    "    big_X_imputed[feature] = le.fit_transform(big_X_imputed[feature])\n",
    "\n",
    "# Prepare the inputs for the model\n",
    "train_X = big_X_imputed[0:train_df.shape[0]].as_matrix()\n",
    "test_X = big_X_imputed[train_df.shape[0]::].as_matrix()\n",
    "train_y = train_df['label']\n",
    "print (train_X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "train_X = StandardScaler().fit_transform(train_X)\n",
    "test_X = StandardScaler().fit_transform(test_X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_X, train_y)\n",
    "labels_train = LabelBinarizer().fit_transform(y_train)\n",
    "labels_test = LabelBinarizer().fit_transform(y_test)\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.915105644907\n"
     ]
    }
   ],
   "source": [
    "#0.821885840429\n",
    "#clf = MLPClassifier(random_state=1, max_iter=800, warm_start=True)\n",
    "#350 - 0.810469883318\n",
    "#520 - 0.90403658152\n",
    "\n",
    "#0.894323557237\n",
    "#clf = MLPClassifier(random_state=1, max_iter=800, warm_start=True, early_stopping=True, hidden_layer_sizes=(450,))\n",
    "\n",
    "#0.9\n",
    "#clf = MLPClassifier(random_state=0, max_iter=800, warm_start=True, early_stopping=True, hidden_layer_sizes=(520,))\n",
    "\n",
    "#0.903752759382, 0.911321349732, 0.910848312835, 0.91160517187\n",
    "clf = MLPClassifier(random_state=None, max_iter=800, algorithm='l-bfgs',\n",
    "                    warm_start=False, shuffle=False, alpha=1e-9,\n",
    "                    early_stopping=False, hidden_layer_sizes=(550,))\n",
    "\n",
    "#0.890665405235, 0.913844213182\n",
    "#clf = MLPClassifier(random_state=1, max_iter=800, warm_start=True, early_stopping=True, hidden_layer_sizes=(560,))\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print (score)\n",
    "\n",
    "predictions = []\n",
    "predictions=clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "indx=[x for x in range(1,31710)]\n",
    "submission = pd.DataFrame({'Id': indx,\n",
    "                           'Prediction': predictions })\n",
    "submission.to_csv(\"submission.csv\", index=False, sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
